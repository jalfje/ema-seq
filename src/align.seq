from fixes import *
## align.seq
## Script which performs EMA alignment.

import math
import bio.bwa as seq_bwa

from data import *
from samdict import *
from bwabridge import *
from input_reader import InputReader
import util

UNPAIRED_PENALTY: float = -15.0
EXTRA_SEARCH_DEPTH: int = 12
INSERT_MIN: int = -35
INSERT_MAX: int = 750

MAX_CLOUDS_PER_BC_LARGE: int = 10000000
MAX_CLOUDS_PER_BC_SMALL: int = 1000000
EM_ITERS: int = 5
INDEL_RATE: float = 0.0001
CLIP_RATE: float = 0.03

# Check if sam records are a pair.
def is_pair(record: SamRecord, mate: SamRecord)-> bool:
    if record.rev == mate.rev or record.chrom != mate.chrom:
        return False

    if mate.rev:
        tmp = mate
        mate = record
        record = mate

    d = record.pos - mate.pos
    return INSERT_MIN <= d and d <= INSERT_MAX

# Get penalty for mate pair distance.
def mate_dist_penalty(mate1_pos: int, mate2_pos: int):
    d = mate1_pos - mate2_pos
    if INSERT_MIN <= d and d <= INSERT_MAX:
        return 0.0
    else:
        return UNPAIRED_PENALTY

def get_alignments(bwa: seq_bwa.BWA, mate1: FastQRecord, mate2: FastQRecord)-> list[SamRecord]:
    alignments: EasyAlignmentPairs = bwa_mem_mate_sw(bwa, mate1.read, mate2.read, 25)
    best_dist = -1
    aligns_added1 = 0
    aligns_added2 = 0

    sam_records = list[SamRecord]()

    is_first = True
    for a in alignments.a1:
        r: SingleReadAlignment = bwa_smith_waterman(bwa, mate1.read, a.chained_hit)

        clip = len(mate1.read) - (a.read_end - a.read_start)

        if clip >= len(mate1.read) / 2:
            continue

        dist = r.edit_dist + clip
        if is_first:
            best_dist = dist
            is_first = False
        elif (dist - best_dist) > EXTRA_SEARCH_DEPTH:
            continue

        r.mapq = mem_approx_mapq_se_insist(bwa, a.chained_hit)
        sam_records.append(SamRecord(mate1, mate2, r, False, clip, dist))
        aligns_added1 += 1

    if aligns_added1 == 1:
        sam_records[-1].unique = True

    is_first = True
    for a in alignments.a2:
        r: SingleReadAlignment = bwa_smith_waterman(bwa, mate2.read, a.chained_hit)

        clip = len(mate2.read) - (a.read_end - a.read_start)

        if clip >= len(mate2.read) / 2:
            continue

        dist = r.edit_dist + clip
        if is_first:
            best_dist = dist
            is_first = False
        elif (dist - best_dist) > EXTRA_SEARCH_DEPTH:
            continue

        r.mapq = mem_approx_mapq_se_insist(bwa, a.chained_hit)
        sam_records.append(SamRecord(mate2, mate1, r, True, clip, dist))
        aligns_added2 += 1

    if aligns_added2 == 1:
        sam_records[-1].unique = True

    return sam_records

# Recompute gammas for a SamDictEntry in-place, returning the maximum gamma change
def recompute_gammas(e: SamDictEntry, tech: PlatformProfile)-> float:
    mate_records = list[SamRecord]()
    mate_clouds = list[Cloud]()
    mate_gammas = list[float]()

    if e.mate is not None:
        mate_records = e.mate.cand_records
        mate_clouds = e.mate.cand_clouds
        mate_gammas = e.mate.gammas

    cloud_weights = list[float]()

    # With many clouds, normalize probabilities for each read individually
    if tech.many_clouds:
        cloud_weights = [c.weight for c in e.cand_clouds]
        total_weight = sum(cloud_weights)
        cloud_weights = [w / total_weight for w in cloud_weights]

    old_gammas = copy(e.gammas)

    for (idx, (r, c, cw)) in enumerate(zip3(e.cand_records, e.cand_clouds, cloud_weights)):
        best_mate_score: float = UNPAIRED_PENALTY

        if e.mate is not None:
            for mr, mc, mg in zip3(mate_records, mate_clouds, mate_gammas):
                if mr.chrom == r.chrom and mr.rev != r.rev and c is mc and mg != 0.0:
                    penalty = mate_dist_penalty(r.pos, mr.pos) if r.rev else mate_dist_penalty(mr.pos, r.pos)
                    mate_score = penalty + math.log(mg)
                    if mate_score > best_mate_score:
                        best_mate_score = mate_score

        e.gammas[idx] = r.score + (math.log(cw) if tech.many_clouds else c.weight) + best_mate_score

    util.normalize_log_probabilities(e.gammas)

    max_change = max([abs(old - new) for old, new in zip(old_gammas, e.gammas)])
    return max_change


SAM_READ_PAIRED = 1
SAM_READ_PROPER = 2
SAM_READ_UNMAPPED = 4
SAM_MATE_UNMAPPED = 8
SAM_READ_REVERSED = 16
SAM_MATE_REVERSED = 32
SAM_1ST_IN_PAIR = 64
SAM_2ND_IN_PAIR = 128
SAM_READ_IS_DUP = 1024
def sam_record_string(record: SamRecord, mate: SamRecord, read_group_id: str, bx_index: str)-> str:
    assert(record is not None or mate is not None)

    flag: int = SAM_READ_PAIRED
    ident: str = ""
    chrom: str = "*"
    pos: int = 0
    mapq: int = 0
    read_len: int = 0
    barcode: int = 0
    r: SingleReadAlignment = None
    fq: FastQRecord = None

    alts = list[Alt]()
    gamma: float = 0.0
    cloud: Cloud = None

    if record is not None:
        ident = record.ident
        chrom = chrom_lookup(record.chrom)
        pos = record.pos
        read_len = len(record.fq.read)
        barcode = record.barcode
        r = record.alignment
        fq = record.fq

        alts = record.alts
        gamma = record.gamma
        cloud = record.cloud

        assert(not math.isnan(gamma))

        gamma_mapq = int(-10.0 * math.log10(1.0 - gamma)) if gamma <= 0.999999 else 60
        score_mapq = record.score_mapq
        bwa_mapq = record.mapq
        mapq = min(gamma_mapq, score_mapq, bwa_mapq, 60)
        mapq = max(mapq, 0)

        if record.rev:
            flag |= SAM_READ_REVERSED
        if record.duplicate:
            flag |= SAM_READ_IS_DUP

        flag |= SAM_1ST_IN_PAIR if not record.mate else SAM_2ND_IN_PAIR
    else:
        ident = mate.ident
        read_len = len(mate.fq_mate.read)
        barcode = mate.barcode
        fq = mate.fq_mate
        flag |= SAM_READ_UNMAPPED
        flag |= SAM_1ST_IN_PAIR if mate.mate else SAM_2ND_IN_PAIR

    if mate is not None:
        if record is not None and is_pair(record, mate):
            flag |= SAM_READ_PROPER
        if mate.rev:
            flag |= SAM_MATE_REVERSED
    else:
        flag |= SAM_MATE_UNMAPPED

    ## Start outputting
    out_str = ""

    # Basics
    out_str += f"{ident}\t{flag}\t{chrom}\t{pos}\t{mapq}\t"

    # Cigar
    if record is not None:
        out_str += str(r.cigar)
    else:
        out_str += "*"

    # Mate mapping
    if mate is not None:
        same_chrom = record is not None and mate.chrom == record.chrom
        out_str += f"\t=\t{mate.pos}\t" if same_chrom else f"\t{chrom_lookup(mate.chrom)}\t{mate.pos}\t"

        s: SingleReadAlignment = mate.alignment
        if same_chrom and len(r.cigar) > 0 and len(s.cigar) > 0:
            p0 = r.pos + ((r.cigar.rlen - 1) if r.rev else 0)
            p1 = s.pos + ((s.cigar.rlen - 1) if s.rev else 0)
            sign = 1 if p0 > p1 else (-1 if p0 < p1 else 0)
            out_str += str(-(p0 - p1 + sign))
        else:
            out_str += "0"
    else:
        out_str += "\t*\t0\t0"

    # Sequence and Quality scores
    out_str += "\t"
    if record is not None and record.rev:
        out_str += str(~fq.read)
        out_str += "\t"
        out_str += fq.qual[-1::-1]
    else:
        out_str += str(fq.read)
        out_str += "\t"
        out_str += fq.qual

    ## Tags
    # Miscellaneous, including barcode
    bc_str = util.decode_barcode(barcode)
    if record is not None:
        out_str += f"\tNM:i:{r.edit_dist}\tBX:Z:{bc_str}-{bx_index}\tXG:f:{gamma}\tMI:i:{cloud.ident}\tXF:i:{1 if cloud.bad else 0}"
    else:
        out_str += f"\tBX:Z:{bc_str}-1"

    # Read group
    if len(read_group_id) > 0:
        out_str += f"\tRG:Z:{read_group_id}" # read_group_id should have no trailing whitespace

    # Alternate mappings
    if len(alts) > 0:
        out_str += "\tXA:Z:"
        for alt in alts:
            out_str += f"{alt.chrom},{'-' if alt.rev else '+'}{alt.pos},"
            out_str += str(alt.cigar)
            out_str += f",{alt.edit_dist};"

    out_str += "\n"
    return out_str

# TODO: Refactor & extend this to handle multiple cases for file inputs and threads:
# Threads:
# - 1 thread per file (or file pair), with threading handled outside this function [existing/submitted behaviour]
# - multiple threads per file (or file pair) [behaviour available in EMA]
# File inputs: (DONE as of 2020-12-26)
# - one (or multiple) specially-formatted (via EMA preprocessing) fastq file(s) [existing/submitted behaviour; -s -x flags]
# - two preprocessed & sorted standard fastq files (one for each end of paired-end reads) [EMA -1 and -2 flags]
# - one preprocessed & sorted interleaved standard fastq file (i.e. both ends of paired-end reads subsequently) [EMA -1 flag]
# Future considerations:
# - why not multiple preprocessed & sorted interleaved standard fastq files? seems viable; maybe it's not relevant
# - why not multiple pairs of standard fastq files? seems viable (tho input would get slightly messy); maybe it's not relevant
# - taking one of the above from standard input? --> sorting is required. but what if we assumed it was sorted?
#   - if we assumed it was sorted, we could do interleaved or specially-formatted inputs. that seems sensible.
#     - specially-formatted inputs assume that they aren't sorted. this could still be from stdin (so that the interim file
#       doesn't need to be written to disk; and for general flexibility), but it wouldn't be able to process it as a stream
#       very effectively, because it would need to wait for the end of the file until it could do anything with it.

# Thoughts/Notes:
# Everything in the function is local to the thread, except input/output files, and possibly some global variables.
# I think the only global variable that is really not threadsafe is the Cloud constructor, which increments a global.
# I don't know if there's any way around it, but there probably is if seq provides some form of mutex.
# mutex: https://docs.seq-lang.org/stdlib/threading.html i.e. "import threading" and then "mutex = threading.Lock()"
# followed by "mutex.acquire()" and "mutex.release()".
def find_clouds_and_align(bwa: seq_bwa.BWA, in_reader: InputReader, out_file, apply_opt: bool, tech: PlatformProfile, threads_per_file: int, read_group: str, bx_index: str):
    # TODO Future work: parallelize across threads_per_file threads

    max_clouds = MAX_CLOUDS_PER_BC_LARGE if tech.many_clouds else MAX_CLOUDS_PER_BC_SMALL
    read_group_id = str() if len(read_group) == 0 else read_group[read_group.index("ID:")+3:].split()[0]
    init_alignment_score_weights(tech.error_rate, INDEL_RATE, CLIP_RATE)
    init_bwa_pes() # from bwabridge

    while True:
        ### Get barcode group (first value of returned tuple is False if there is no more input)
        barcode_group_data = in_reader.get_next_barcode_group()
        good = barcode_group_data[0]
        barcode = barcode_group_data[1]
        read_pairs = barcode_group_data[2]
        if not good:
            break

        ### Initialize
        sam_dict = SamDict(tech)
        clouds = list[Cloud]()
        worth_doing_full_em = len(read_pairs) >= 30

        ### Generate list of alignments for reads in this barcode group
        sam_records = [aln for mate1, mate2 in read_pairs for aln in get_alignments(bwa, mate1, mate2)]
        sam_records = sorted(sam_records, key=sam_record_cmp_key)

        ### Find and process clouds
        r_idx = 0
        while r_idx < len(sam_records) and sam_records[r_idx].barcode == barcode:
            r_start_idx = r_idx
            assert(len(clouds) < max_clouds)
            cloud = Cloud()
            clouds.append(cloud)

            sam_dict.add(sam_records[r_idx], cloud, force=False)
            cov = 1
            collision_detected = False
            while (r_idx+1 < len(sam_records) and
                   sam_records[r_idx+1].barcode == sam_records[r_idx].barcode and
                   sam_records[r_idx+1].chrom == sam_records[r_idx].chrom and
                   sam_records[r_idx+1].pos - sam_records[r_idx].pos <= tech.dist_thresh):
                r_idx += 1

                if not collision_detected and sam_dict.add(sam_records[r_idx], cloud, force=False):
                    collision_detected = True
                    for idx in range(cov):
                        sam_dict.delete_last(sam_records[r_start_idx+idx])

                cov += 1

            if collision_detected:
                cloud.bad = True
                cloud_to_split = sorted(sam_records[r_start_idx:r_start_idx+cov], key=sam_name_cmp_key)

                # This is the -d flag. Not implementing for the project.
                #if apply_opt:
                #    mark_optimal_alignments_in_cloud(cloud_to_split)

                for rec in cloud_to_split:
                    sam_dict.add(rec, cloud, force=True)

            r_idx += 1

        ### Initializations
        for e in sam_dict.entries():
            util.normalize_log_probabilities(e.gammas)

            for cloud, gamma in zip(e.cand_clouds, e.gammas):
                cloud.exp_cov += gamma

        # Compute cloud scores
        for cloud in clouds:
            cloud.weight = cloud.exp_cov

        if not tech.many_clouds:
            normalize_cloud_probabilities(clouds)

        ### Expectation-Maximization iterations
        for q in range(EM_ITERS):
            if not worth_doing_full_em:
                break

            # Clear current weights
            for cloud in clouds:
                cloud.exp_cov = 0.0

            max_change = 0.0
            # Recompute gammas
            for e in sam_dict.entries():
                entry_max_change = recompute_gammas(e, tech)
                max_change = max(entry_max_change, max_change)

            # Update clouds
            for e in sam_dict.entries():
                for r, c, g in zip3(e.cand_records, e.cand_clouds, e.gammas):
                    if r.active and not r.duplicate:
                        c.exp_cov += g

            # Recompute cloud scores
            for cloud in clouds:
                cloud.weight = cloud.exp_cov

            if not tech.many_clouds:
                normalize_cloud_probabilities(clouds)

        records_final = list[SamRecord]()
        ### Find best alignments
        for e in sam_dict.entries():
            if not e.visited:
                m: SamDictEntry = e.mate
                best: SamRecord = e.find_best_record()
                best_mate: SamRecord = m.find_best_record() if m is not None else None

                best.selected_mate = best_mate
                records_final.append(best)

                if best_mate is not None:
                    best_mate.selected_mate = best
                    records_final.append(best_mate)

                e.visited = True
                if m is not None:
                    m.visited = True
                    m.mate = None

        ### Skip duplicates
        if not tech.many_clouds:
            records_final = sorted(records_final, key=sam_duplicate_cmp_key)
            n_records_final = len(records_final)

            i = 0
            while i < n_records_final:
                j = i + 1
                while j < n_records_final and sam_duplicate_cmp_key(records_final[i]) == sam_duplicate_cmp_key(records_final[j]):
                    records_final[j].duplicate = True
                    j += 1
                i = j

        ### Print records
        for record in records_final:
            if record.visited:
                continue

            mate = record.selected_mate
            if mate is not None:
                mate.visited = True

            out_file.write(sam_record_string(record, mate, read_group_id, bx_index))
            out_file.write(sam_record_string(mate, record, read_group_id, bx_index))

